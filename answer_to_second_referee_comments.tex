We thank the reviewer for his/her helpful comments and apologize for the delay in returning a revised version of the manuscript. Large blocks of changed text in the manuscript appear in bold (we have also made minor grammatical and style changes which are not marked). Our responses to individual comments are in the text below, beginning with ---

%TBD: make sure to delete comments in this file before submitting!


Reviewer's Comments:

This paper proposes the use of self-organizing maps (1D and 2D) to aid in classifying galaxy spectra.  The revision has addressed several points and improve the clarity of the text.  However, there are several remaining issues.

1. The authors did not address my comment about the significance of the work.  What was learned?  What method is advocated, and what is the evidence for its superiority?

The sentence that was added about 1D vs. 2D maps comes very late in the paper and is quite abstract, stating that the two methods are complementary.  The Introduction should establish the goals of the investigation so the reader understands the purpose of each experiment as well as its significance.

--- We have reorganized the last two paragraphs of Section 1 and added text to better describe the goal of the investigation: comparing different methods of classifying galaxy spectra against templates, with a focus on self-organizing maps.
We have added additional text to Sections 4.2 and 5 comparing 1D and 2D SOMs.
We have also added some text to Section 5 discussing the significance of the work. 


The paper claims that "Compared to previous attempts to classify this sample, the self-organizing map approach was able to better classify the spectra which had similarities with more than one template." (abstract), but I cannot find a quantitative comparison with any previous results.  Figure 9 is presented as the result to compare with previous work, but since the previous results are not presented nor summarized, the reader can't tell how they differ.  The paper reports "more precision", but this is not defined; does it mean smaller error bars?  Stronger correlation?  How does this show "better classification"?  To support such a claim, this paper needs to select a performance metric and report on the values achieved in these experiments, compared to other methods.
--- We selected several performance metrics. Two of them (an agreement score and the Fleiss kappa score) measure the degree to which the classifications agree. 
% TBD: have not done the following yet
As a measure of the classification performance, we calculate the dispersion in properties of the galaxies in each class and compare these values between methods. The results of this analysis are discussed in Section 4.4.

I had suggested comparing the results to a simple nearest-neighbor classification baseline, but this was not addressed.
--- We apologize for missing this helpful suggestion in the previous version. This is now part of the analysis described in the paragraph above; the results appear in Section 4.4.

The comment about how SOMs provide an ordered clustering of the data (with respect to feature values), which k-means does not, is certainly true.  But the paper must then explain why this is important or valuable.
--- We have added text to the end of both Section 4.2 (formerly section 4.1) and Section 5 explaining the utility of an ordered clustering.

2. Perhaps I have missed it somewhere, but I don't think the paper ever states how exactly the classification of new data is done.  As I understand it, the authors train an SOM using template spectra, then classify new spectra by determining which SOM node is the best fit for the new item and classifying it based on the labeled template spectra that also reside in that node -- but this isn't actually stated anywhere.  Please add text explaining the classification step.  This would probably go in the final paragraph of the introduction and/or a new subsection in section 3 ("Method") - this section describes SOMs and k-means but not how to use either model to classify new data. (i.e., the SOM can't classify anything without knowing the labels of its original data set.  So although it builds the SOM in an unsupervised way, it requires the supervision of the template labels to be able to assign a label to a new item).
-- Added the following paragraph  at the end of section 3.2: 
    "As mentioned in the introduction, one of the main advantages of the SOM method  is that the resulting networks can be used to cluster new datasets with no additional training required. 
     New data with the same dimensionality as the original input data can be presented to the already trained network: the SOM algorithm finds the best matching unit considering the weight of nodes in the trained network and each vector from the new data set.
     The winner node determines the place of each new vector on the SOM.
     The location of the new vectors on the SOM allows them to be compared with the original data set."

On page 2, lines 29-31, the authors added this text: "An SOM can be trained without supervision and its results used to classify other data. Therefore, this method can be considered as (semi)-supervised." As written, the phrase "this method" seems to refer to SOMs (so the statement wouldn't be true - SOMs are fully unsupervised), but I think instead the authors' proposed method (of building an SOM and then applying it to classify new data, using labels on the SOM-grouped data) is what is meant.  Please clarify.
--- Changed to: 
    "The training of a self-organizing map is fully unsupervised.
     Using the resulting map as a template to classify other data requires labelling the output, which is why some groups consider self-organizing maps to be a (semi)-supervised method."

3. The inclusion of k-means helps give useful context to this work. Figure 10 shows a head-to-head comparison of k-means and SOM applied to the 12 templates using 4 clusters/nodes.  But I can't tell which one is better.  The text states that they are similar but "show discrepancies for spectra types between the extreme cases".  Which ones?  How can I tell from this figure?
--- The text describing the differences between the K-means and SOM classification of the templates (in particular, which templates are grouped together by the two methods) has been rewritten and moved to Section 4.1 (newly added in this revision). 

At the bottom of page 10, the paper states: "The other difference between these two methods is that the network created by the SOM method can be used to classify the other spectra, while for the K-means clusters they can only be used for one set of data."  This isn't true.  (The paper itself points out that SOM reduces to k-means if the weight on the neighborhood function goes to 0, so you should be able to use the result in the same way.)  You can (and should, to do a direct comparison) use the clusters just like you used the SOM nodes - for a new data point, assign it to the most similar cluster center, then assign the template label from the original data used to construct the clusters.
--- We thank the referee for this helpful suggestion and have implemented it in our use of K-means. The text has been changed to remove this incorrect statement.

To analyze the 142 galaxies, the SOM and k-means experiments used different methodology and so are difficult to compare or understand. It seems that for k-means, the authors clustered the 142 sample galaxies and then classified the mean spectrum in each cluster and reported how well that did.  But for the SOM, the authors constructed the SOM based on the 12 templates from K96 and then assigned classes to individual new spectra (not the nodes).  This doesn't seem comparable.  Please use the same methodology in both cases.
--- This is a reasonable criticism and we have changed our methodology to match between methods. We now use the templates to generate the initial classification in both cases, and then apply that classification to the 142 sample galaxies.
%TBD: Sahar please check.


Further, k-means and the SOM used different distance functions (Euclidean vs. spectral angle distance) when operating on the same data.  Differences in results could simply be due to this change.  The authors' explanation for this choice does not make sense to me.
--- We have changed to using the Euclidean distance in both cases.
%TBD: Sahar please check.

The text says that Figure 11 shows "the best and the worst fitting results among the 22 clusters" (for k-means) but instead it seems to show the best-fit for two clusters (9 and 22) which both look pretty good.  The text implies that Fig 11 (right) is a poor match, but it looks very good.  Please explain.
--- We apologize for the confusion. As we have substantially changed the comparison between K-means and SOM results, this figure is no longer relevant and has been removed.


4. If k-means is retained, the way it is presented needs improvement. The description of k-means clustering in section 1 is a nice addition, but the comparison between k-means and SOM is confusing and wordy. K-means optimizes a partition of the data based on distance in the feature space only.  SOMs operate on a combination of distance in feature space and distance in the resulting low-dimensional map.  I am not sure how this achieves "smoothing", nor whether this is an important detail (the paper never mentions it again).  I recommend explaining what is meant here and why a "smooth" solution is better, or omitting this detail.  Explaining how SOM can be reduced to k-means is a good addition.
--- We have rewritten this paragraph (third to last of Section 1) and omitted the discussion of smoothing. 

Please a add citation to the original k-means algorithm (e.g., MacQueen, 1967) when it is first mentioned (in addition to citing people who've used the algorithm).
--- Added the proper citation.

Section 3.3 outlines the k-means method that is used in the experiments.  The language here appears to be more informal and less polished than that which describes the SOM method.  To explain k-means with the same level of detail, it would be appropriate to show the objective function that is employed and to explain that it uses an alternating optimization to assign points to centroids and then update the centroids, iterating until... what?  (There are different termination conditions used by different implementations, so it would be useful to indicate here what the matlab method does.)

--- We have reorganized Section 3 to discuss K-means first, followed by SOM. The former section 3.3 (now 3.1) has been rewritten, giving the objective function and using more formal language. The termination condition is indicated in the text.

