Reviewer's Comments:

This paper proposes the use of self-organizing maps (1D and 2D) to aid in classifying galaxy spectra.  In contrast to strictly supervised methods, the SOM is able to create its own classes including those that may fall "between" existing template classes, and therefore it can provide a better fit to the data.

This is a very clear, readable, and well written paper.  The careful steps through different network architectures helps the reader understand the different behavior and results.

However, what's missing is a larger sense of significance.  What did this analysis accomplish?  The paper claims value in having "classified" all 142 samples.  But any arbitrary decision rule could classify them (i.e., assign them to a class); what is needed is some statement about the accuracy of that classification, or efficiency, or some other measurable benefit.  Related, it is confusing to claim that

"Most current classification methods cannot classify SEDs outside the range of templates, and will fail in special cases." (abstract)

Any classification method can generate a classification for any input.  It may be the case that this is of low confidence, or it is incorrect, but there is still some output.  I suggest rephrasing this claim to indicate the actual limitation (low accuracy? low confidence? something else?).

In the end, how does one use the output of the SOM classification? Can you evaluate its accuracy in some way?  I understand that SOMs are exploratory and unsupervised, but there should be some demonstrable benefit or value from having done it.

The only statement I can see about something learned from the exercise is that the SOM yielded results with higher correlations between non-spectral properties.  But correlation is never quantified; instead plots are shown, leaving the reader's eye to estimate correlation.  To my eye, the plots in the lower row of Figure 10 have low correlation, if any.  What is the R^2 value?  And why does this matter?

Other major suggestions:

1. Motivate the use of 1D versus 2D SOMs (or both).  As currently presented it is not clear what the benefit is of using either one, especially since they give different results.  Which one is better? Since they have different results, what does each one tell us?  At the end of section 4, the paper advises investigators to use the 2D approach first.  Why?  And if so, what additional knowledge is gained by applying the 1D approach?

2. Although the paper explains why the SOM has more flexibility/expressivity than the neural network approach in reference T12, it does not compare the SOM to any other method out there.  The obvious candidate for comparison would be to apply k-means clustering (e.g., cluster the 142 spectra and then each clusters to its closest match of the original 12 templates).  But to compare the meaningfully, again you'll need some measure of performance, which currently is lacking.

Relatedly, what performance does one get simply by assigning each of the 142 spectra to its closest match from the 12 templates?  This would help us understand whether an SOM is even needed for this problem.

Minor points:

- The "mock sample" in section 3.3 does not add anything to the paper.  I suggest omitting it.  It raises more questions than it answers.  For example, is this randomly generated data?  Why 27 samples? Apparently there are only two features with only two possible values each, so there are only 4 unique possible samples, so 23 of the 27 must duplicate some other sample, and the data set is strongly over-specified.  Further, the features have values that are categorical rather than numeric (e.g., "high" and "low"), and it is not at all clear how the SOM, which employs Euclidean distance, can handle that kind of data.  Being able to divide a data set with only 4 unique samples into 4 distinct groups seems to be a non-accomplishment.

- I could not understand how the 2D SOM was constrained so that the results in the right-hand side of Figure 11 were obtained.  This network has the same topology as the one on the left yet somehow the K96 templates "are in a small region" (p. 10, line 43, column 2).

- Euclidean distance is not the best way to match spectra; I recommend using spectral angle distance instead.

- This isn't a fully unsupervised appraoch, since the SOM results are "classified" using the 12 template types (supervision).  That's not a weakness, but it means it would be more accurate to refer to the  method as "weakly supervised" or "partially supervised."

- Section 4.1.1 (p. 6, line 37-39, column 2) states that "The black color indicates that the left neuron is completely different from the other two groups" but earlier it was explained that these colors are relative to the range of differences present.  So black and white could be quite close (e.g., 0.1 and 0.2) or they could be large (e.g., 1 and 100000) but there's no way to know from the color.

- Section 4.1.1 (p. 7, lines 10-11, column 1) confusingly states that Figure 5 shows that "5 groups on the left-side neurons are separated from 7 groups on the right side of the map with two dark-grey colors between them."  There are indeed 5 groups then 2 dark-grey links then 7 groups, but those 7 are broken into sub groups with even greater contrast (black links).  So it would be more appropriate to say that the galaxies are split as 5 - 1 - 1 - 5 or even 6 - 1 - 6.

- Section 4.1.2 (p. 7, line 9, column 2) says that Figure 8 has 12 plots, but actually it has 14.

- It is hard for the reader to track K96, T12, and N09 when their definitions are embedded pages before they are referenced.  I suggest including a small table with these labels, what they are, and their full citation so readers can quickly understand what is meant.

- When re-using figures from a previous publication, you may need copyright permission from the journal source (e.g., Figure 1 was published by the Astronomical Journal).