%Referee comments
%1d. The authors stated "We have added a footnote to clarify how chi-squared was computed."  I cannot find this footnote.  Please define this method in section 3.

%1e. The metrics proposed in section 3.4 are
%- agreement of algorithm X's output with that obtained by a chi-squared minimization
%- Fleiss kappa
%- silhouette score (S) - also referred to as within-cluster similarity

%I suggest referring to the first metric as "chi-squared agreement" (here and throughout, including the discussion of results starting in section 4.3) or some other term to capture the fact that it specifically relates an algorithm to the chi-squared results.

%I am still confused by the the Fleiss kappa results.  If the chi-squared method is the point of reference to use (as with the proposed agreement metric), why does it not appear here?  What does it tell us to determine that the k-means, supervised ANN, and SOMs are in "fair" agreement? (with each other?)  Is that a goal?  The chi-squared agreement analysis showed higher agreement with the SOM than with the other methods, so I wouldn't expect (nor want) there to be high kappa values between those three.  As currently presented, I don't think this adds anything to the paper, and I suggest omitting it (or rephrasing as a second measure of agreement with chi-squared, and computing values accordingly).

%2. The re-organization of the paper is helpful for the Introduction, but the new content needs to be better integrated into sections 3.1 and 3.2.  Currently it reads disjointedly.

%2a. Section 3.1: The notation "N \in R^m" does not make sense, especially since m is not defined.  Consider something like "N objects, each represented by m features" or "N objects x \in R^m, where m is the number of features used to represent each object".

%Note that the following paragraph uses a lower-case "n" for the number of objects.  Please make these consistent.

%2b. Section 3.2:
%- The text prior to 3.2.1 is now rather unwieldy and hard to follow.  It jumps back and forth between ANNs and SOMs. I suggest making a new 3.2 that covers "Artificial neural networks" (since the paper treats them as an alternative to SOMs later) and 3.3 can be devoted to SOMs as a specialized (semi-supervised) version of ANNs.  Merge old and new content to make a logical flow.
%- Provide a citation for ANNs when first mentioned.
%- What is meant by "a set of training methods" (end of page 3, column 2)?  Typically only a single training method (e.g., backpropagation) is used.
%- Standard practice is to avoid having a single subsection (3.2.1). Remove the subsection heading, or make two (3.2.1, 3.2.2) on different sub-topics.  For example, section 3.3 could (should) be merged into the SOM section, as a subsection.

% from minor ref comments:
%2. The paper now asserts that ANNs outperform chi-square methods (top of page 4, left column) - in what sense?  Accuracy (which as we've discussed may not be measurable)?  Runtime?  Something else?

\section{METHOD}
\label{sec: method_somz}

In this section we  briefly review the use of both self-organizing maps and K-means clustering in astronomy and describe their algorithms. 
As K-means is both simpler and more well-known, we introduce it briefly first before discussing self-organizing maps in detail. 

\subsection{K-means clustering}
\label{sec: kmeans_method}


K-means clustering~\citep{Macqueen67} is an unsupervised method used in many astronomical studies~\citep[e.g.][]{DAbrusco12,Ordov14,Boersma14,Aycha16}.
K-means clustering divides data into $K$ clusters, in such a way that each data point belongs to the cluster with the nearest mean value.
To first order, SOM and K-means work similarly: both algorithms operate on the distances between \boldit{N} $\in \Re^m$ objects defined by the properties under consideration.
The SOM algorithm also considers distances between objects in the low-dimensional map, as controlled by a neighborhood function. 
When the radius of the neighbourhood function goes to zero, the SOM algorithm loses its ordering power and acts like K-means clustering.



The K-means method partitions a data set $x_i$, having $n$ points in $d$-dimensional feature space, into $K$ clusters.
The number of clusters $K\leq n$ must be decided by the user in advance, and the cluster centroids in feature space are located at positions $\mu_k$.
The goal of the algorithm is to minimize the sum
\begin{equation}
\label{eq:km_sos}
J = \sum_{i=1}^n \sum_{k=1}^K {\min}_k \big( \norm{x_i - \mu_k}^2 \big)
\end{equation}
where $\norm{z}$ indicates the distance measurement in $d$-dimensional space. In this work the Euclidean distance is is used, but other metrics are also possible.
The algorithm is initialized by randomly choosing $K$ points as cluster centroids.
Each data point is assigned to the cluster with the closest centroid, and the centroids are re-calculated.
The steps of re-assigning cluster membership and re-calculating the cluster centroids are repeated until a stopping criterion is reached (a maximum number of iterations or no change in cluster assignments).
We used the \textsc{matlab} K-means library~\citep{Seber84, Spath85}, written for \textsc{matlab2015b}, to perform  K-means clustering.
The default maximum number of iterations (100) was used.



 \subsection{Self-organizing maps}
 \label{sec: som}


 Artificial neural networks (ANNs), which are inspired by the way neurons in a human brain route and process data, are very powerful tools that are used in data processing and pattern recognition problems.
An ANN contains many interconnected units (nodes or neurons) which process data and work together to solve problems.
It uses a set of training methods to learn about nonlinear and complex relations between input and output data, and how to apply these relations to new sets of data~\citep[e.g.][]{Hossein14,Hossein16a,Hossein16b,Ellison16a, Ellison16b}.
Studies have shown that ANNs outperform chi-square minimizing techniques and can be used as an alternative choice for fitting data~\citep[e.g.][]{Marquez91}.
Specifically, ANNs perform faster in large databases~\citep[][]{Gulati97}.



A Kohonen self-organizing map (also called self-organizing map, or SOM) is a (semi)-supervised neural network for mapping and visualizing a complex and nonlinear high dimension data introduced by~\citet{Kohonen82}.
It shows simple geometrical relationships in non-linear high dimensional data on a map \citep{Kohonen98}.
The training of a self-organizing map is fully unsupervised.
     Using the resulting map as a template to classify other data requires labelling the output, which is why some groups consider self-organizing maps to be a (semi)-supervised method.
The utilization of the SOM in astronomy dates back to the 1990s, with \citet[][]{Odewahn92}, \citet[][]{Hernandez94}, and \citet[][]{Murtagh95} among the first to use SOMs in their studies.
\citet{Geach12} used COSMOS data to demonstrate two of the main applications of SOMs: object classification and clustering, and photometric redshift estimation. The latter has been the subject of many other studies \citep[e.g.][]{Kind14a}.
From classifying quasars' spectra to star/galaxy classifications, from gamma-ray burst clustering to classification of light curves, this method has proved to be useful in various fields of astronomy \citep[e.g.][]{Maehoenen95, Miller96, Andreon00, Balastegui01, Rajaniemi02, Brett04, Scaringi09}.



Large spectroscopic surveys have made available integrated spectra of millions of galaxies.
These integrated spectra combine the light of billions of individual stars and nebulae within a galaxy, and
finding patterns and common characteristics between galaxies can be a complex task.
\citet{In12} introduced a new clustering tool based on the SOM method for analyzing these large datasets.
They used $\sim 60000$ spectra from the Sloan Digital Sky Survey \citep[SDSS;][]{Abazajian09} to test their tool, and created very large SOMs to analyze the type of spectra/objects.
They also generated SOMs from quasars' spectra in order to find unusual types of spectra. 
Later, \citet{Meusinger16} used these SOMs and updated data from SDSS and other surveys to find a new class of quasars.
The other application of SOMs is to find outliers or errors in the data.
\citet{Fustes13} produced a package based on SOM to classify spectra from the GAIA survey that were previously classified as ``unknown'' by the SDSS pipeline. This package can distinguish an astronomical object from instrumental artifact, and then classify the object based on its spectrum.


 
 The self-organizing map is a clustering method which reduces the dimensionality of the data, usually to one or two dimensions (1D or 2D), while preserving topological features of the original dataset.
 The result of an SOM is a set of nodes (neurons) that are arranged in a 1D or 2D arrays \citep{Kohonen98}. 
 Each node may contain one or more samples from the input data.
 The distance between the nodes represents similarity or dissimilarity of the underlying samples, i.e., similar data are closer together in the array and the distance between two nodes is related to the dissimilarity of their samples.
 A weight vector ``\boldit{W}" with the same dimension as the input data is associated with each node and will be varied during the training process.
 This vector is the key factor in determining the position of the nodes in a map.
 \cite{Geach12} presented the application of the self-organized map and discussed its algorithm in detail.
 In this section we briefly discuss the algorithm of SOM and how we create our maps. 
 
 \subsubsection{SOM algorithm} 
 \label{sec: algorithm}
     Assume we have a dataset which contains vectors, \boldit{V} $\in \Re^d$, and we want to map them on an S1 by S2 map. 
     Sizes of SOMs are arbitrary and there are no rules regarding choosing one over the other. 
    \citet{Vesanto05} suggested that a total number of $5\sqrt{d}$ neurons is a sufficient size, but users usually choose the size of the grids based on their dataset and their application of the results.

     We start by creating S1 $\times$ S2 empty neurons. 
     The arrangement of these neurons depends on the map's topology provided by the user. 
     In the case of 1D maps, since each neuron has two immediate neighbours, the topology of the map does not have any effect on the final result and any topology can be chosen.
     However, in 2D maps, the shape of the neurons specifies the number of immediate neighbours for each neuron and it is up to user to choose the most suitable shape based on the data.
     In this paper, we choose hexagonal topology, which gives each neuron six neighbours, and provides more interactions between neurons.
     Initially a random weight vector, \boldit{W} $\in \Re^d$, will be assigned to each node.
     The process of creating SOM happens over a series of $N$ iterations, where $N$ is set by the user. 
     During each iteration the weight vectors might change according to the Kohonen learning rule (equation~\ref{equ: weight adj}). 
      In each iteration, the SOM code:
     \begin{enumerate}
        \item chooses a random vector from the dataset ($V_i$).
        \item calculates the Euclidean distance in $\Re^d$ space for each node, $j$, as  $D_j^2= \sum_{i=0}^{i=d} (V_i - W_{j_i})^2$, and finds a neuron with minimum $D_j$, (``$D_{j_{\min}}$"). This neuron is the winning node and is called the Best Matching Unit (BMU). 
        \item  computes the radius of the neighbourhood of the BMU to find nodes within this radius. The weight vectors of these nodes will be affected in the next steps. The radius of the neighbourhood is arbitrary and can be set to be as high as half of the SOM size. It then decays exponentially over each iteration as
        \begin{equation}
            r^t_{\rm BMU} = r^0_{\rm BMU}e^{(-t/\tau)}
        \end{equation}
        where $\tau$ is a decay constant and is usually set to be the same as the number of iterations, $N$. $r^0_{BMU}$ and $r^t_{\rm BMU}$ are the radii of the neighbourhood at 0th and $t$th iteration, respectively. 
        \item changes the weight vectors of the BMU and all the nodes within $r^t_{\rm BMU}$ as:
        \begin{equation}
            \label{equ: weight adj}
            w(t+1)=w(t)+L(t) \times R(t) \times(v(t)-w(t))
        \end{equation}
        where $L(t) = L_0 e^{(-t/\tau)}$ is the learning factor, which prevents the divergence of the SOM and $R(t)=\exp(-\frac{D_j^2}{2r^t_{\rm BMU}})$ is the influence rate. $R(t)$ determines how the weight of each node in the neighbourhood of BMU will change.
     \end{enumerate}
     These steps are then repeated $N$ times.
     
\subsection{Creating self-organizing maps}
\label{sec: create_som}
     In order to create SOMs, we use the {\sc matlab} neural network toolbox~\citep[NNT,][]{matlabtolbox}, written for \textsc{matlab2015b}. 
     An SOM in {\sc nnt} can be created by the {\sc newsom} or {\sc selforgmap} libraries, both of which work in two phases, an ``ordering phase" and a ``tuning phase". 
     The first phase is called the ``ordering phase" and
     starts with maximum neighbourhood distance and an initial high learning factor (usually 0.9) is provided by the user. 
     The ordering phase continues for a requested number of iterations.
     During the iterations, the learning factor decreases to the tuning phase learning factor and the neighbourhood distance reaches that of the tuning phase as well.
     Both the learning factor and the neighbourhood of the tuning phase are set by the user. 
     The amount by which these two factors change in each iteration depends on the number of iterations.
     
     In the second, or ``tuning'' phase,
     the neighbourhood distance is kept at the user-defined minimum.
     The learning factor, however, decreases gradually.
     The gradual change in the leaning factor helps to fine-tune the topology results, leading to a more stable SOM. 
     To allow the fine tuning, the number of iterations in this phase must be much larger than the that of the ordering phase. 
     We chose the number of epochs in the tuning phase to be 3 times the number of epochs in the ordering phase.
     
     We create the final SOMs with initial values for number of iterations in ordering phase, ordering phase learning factor, tuning phase learning factor, and tuning phase neighbourhood distance of 1000, 0.9, 0.02, and 1, respectively. 
     To present our results, we use {\sc nnt}'s built-in plotting tool.
     Specifically, we use two of the plots in this tool: a hits map, which shows the number of times each neuron has become the winner (hits), and a distance map, which shows the distance between those neurons.
     In the maps, the coloured hexagonal shapes represent the neurons. 
     The distances in a distance map are shown by the grey cycle colours:
     the darker the colour, the larger the distance between neurons.
     In the hit maps, neurons with zero hits are left empty.
     
     As mentioned in the introduction, one of the main advantages of the SOM method is that the resulting networks can be used to cluster new datasets with no additional training required.
     New data with the same dimensionality as the original input data can be presented to the already trained network: the SOM algorithm finds the best matching unit considering the weight of nodes in the trained network and each vector from the new data set.
     The winning node determines the place of each new vector on the SOM.
     The location of the new vectors on the SOM allows them to be compared with the original data set.
     
     \subsection{Classification and clustering metrics}
     \label{sec:metrics}
     
     We use several metrics to measure the degree of agreement between classification methods. 
     As a baseline method for assigning galaxy spectra to template classes, we compute the value of chi-squared between each galaxy and each template%
\footnote{The \citetalias{Kinney96} template spectra do not have tabulated uncertainties and we assume them to be photon-noise dominated for the purposes of computing $\chi^2$.}
     and assign the galaxy to the template class with lowest chi-squared value. 
     A metric of agreement with this classification is an agreement score, computed as follows.
    If a method sorted a galaxy into the same cluster or neuron as its best-fit chi-square template, this was considered a match and scored 1 point. 
    For the larger-sized self-organized maps, if a galaxy was classified between two nodes, one of which contained its best-fit chi-square template, this scored 0.5 point. 
    Otherwise, the classification score was zero points.     
     Another method to check the degree of agreement between different methods is the Fleiss kappa index \citet{landis77}. 
    This index measures agreement between three or more raters that are classifying a specific number of objects to a number of classes and was calculated using the R software package.

        
        
        Classification and clustering are related techniques, and we make
        another comparison between the classification methods with a clustering metric, the silhouette score  \citep{rousseeuw87}.        
This score is a metric used to describe cluster compactness and isolation, given by:
\begin{equation}
S = \frac{1}{n} \sum{\frac{b - a}{\max\big(a, b\big)}}
\end{equation}
where $a$ is the mean distance between a point and the other points in its cluster, and $b$ is the mean distance between a point and the nearest cluster of which that point is not a member.
A higher silhouette score corresponds to a partition with better defined clusters.
For a given dataset, the score generally declines with the number of clusters and is sometime used to determine the optimal cluster number.
In this work we use the silhouette score to compare classifications with a comparable number of classes, as a measure of similarity within the classes.
Mean values for each classification were calculated using the \textsc{scikit-learn} Python package \citep{sklearn}.

     
